{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "# sys.path.remove('/usr/lib/python2.7/dist-packages/caffe/python')\n",
    "sys.path.insert(0, '/home/khuang-ms/kcaffe/python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "caffe_root = '../'  # this file is expected to be in {caffe_root}/examples\n",
    "sys.path.append(caffe_root + 'python')\n",
    "import caffe # If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path.\n",
    "\n",
    "from caffe import layers as L, params as P # Shortcuts to define the net prototxt.\n",
    "\n",
    "sys.path.append(\"pycaffe/layers\") # the datalayers we will use are in this directory.\n",
    "sys.path.append(\"pycaffe\") # the tools file is in this folder\n",
    "\n",
    "import tools #this contains some tools that we need\n",
    "\n",
    "# set data root directory, e.g:\n",
    "pascal_root = osp.join(caffe_root, 'data/VOC/VOCdevkit/VOC2012')\n",
    "\n",
    "# these are the PASCAL classes, we'll need them later.\n",
    "classes = np.asarray(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'])\n",
    "\n",
    "# make sure we have the caffenet weight downloaded.\n",
    "os.path.isfile('/home/khuang-ms/kcaffe/models/FCN-32s/fcn-32s-pascal.caffemodel')\n",
    "\n",
    "# initialize caffe for gpu mode\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(2)\n",
    "\n",
    "workdir = './secondNet'\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)\n",
    "\n",
    "solver = caffe.SGDSolver(osp.join(workdir, 'solver.prototxt'))\n",
    "solver.net.copy_from(caffe_root + '/models/FCN-32s/fcn-32s-pascal.caffemodel')\n",
    "solver.test_nets[0].share_with(solver.net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for itt in range(100):\n",
    "    solver.step(100)\n",
    "    print itt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_baseline_accuracy(net, num_batches, batch_size = 128):\n",
    "    acc = 0.0\n",
    "    for t in range(num_batches):\n",
    "        net.forward()\n",
    "        gts = net.blobs['label'].data\n",
    "        ests = np.zeros((batch_size, len(gts)))\n",
    "        for gt, est in zip(gts, ests): #for each ground truth and estimated label vector\n",
    "            acc += hamming_distance(gt, est)\n",
    "    return acc / (num_batches * batch_size)\n",
    "\n",
    "print 'Baseline accuracy:{0:.4f}'.format(check_baseline_accuracy(solver.test_nets[0], 5823/128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_net = solver.test_nets[0]\n",
    "for image_index in range(5):\n",
    "    plt.figure()\n",
    "    plt.imshow(transformer.deprocess(copy(test_net.blobs['data'].data[image_index, ...])))\n",
    "    gtlist = test_net.blobs['label'].data[image_index, ...].astype(np.int)\n",
    "    estlist = test_net.blobs['score'].data[image_index, ...] > 0\n",
    "    plt.title('GT: {} \\n EST: {}'.format(classes[np.where(gtlist)], classes[np.where(estlist)]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P # Shortcuts to define the net prototxt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L.Python.__doc__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L.Python(module = 'alaska_loss', layer = 'AlaskaLossLayer',  ntop = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(k, v.data.shape) for k, v in solver.test_nets[0].blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.test_nets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "% matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (6, 6)\n",
    "    \n",
    "caffe_root = '../'  # this file is expected to be in {caffe_root}/examples\n",
    "sys.path.append(caffe_root + 'python')\n",
    "import caffe # If you get \"No module named _caffe\", either you have not built pycaffe or you have the wrong path.\n",
    "\n",
    "from caffe import layers as L, params as P # Shortcuts to define the net prototxt.\n",
    "\n",
    "sys.path.append(\"pycaffe/layers\") # the datalayers we will use are in this directory.\n",
    "sys.path.append(\"pycaffe\") # the tools file is in this folder\n",
    "\n",
    "import tools #this contains some tools that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set data root directory, e.g:\n",
    "pascal_root = osp.join(caffe_root, '../../caffe/data/VOC/VOCdevkit/VOC2012')\n",
    "\n",
    "# these are the PASCAL classes, we'll need them later.\n",
    "classes = np.asarray(['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'])\n",
    "\n",
    "# make sure we have the caffenet weight downloaded.\n",
    "os.path.isfile('/raid/yaq007/yaqNIPS/kcaffe/models/FCN-32s/fcn-32s-pascal.caffemodel')\n",
    "\n",
    "# initialize caffe for gpu mode\n",
    "caffe.set_mode_gpu()\n",
    "caffe.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function for common structures\n",
    "def conv_relu(bottom, ks, nout, stride=1, pad=0, group=1):\n",
    "    conv = L.Convolution(bottom, kernel_size=ks, stride=stride,\n",
    "                                num_output=nout, pad=pad, group=group)\n",
    "    return conv, L.ReLU(conv, in_place=True)\n",
    "\n",
    "# another helper function\n",
    "def fc_relu(bottom, nout):\n",
    "    fc = L.InnerProduct(bottom, num_output=nout)\n",
    "    return fc, L.ReLU(fc, in_place=True)\n",
    "\n",
    "# yet another helper function\n",
    "def max_pool(bottom, ks, stride=1):\n",
    "    return L.Pooling(bottom, pool=P.Pooling.MAX, kernel_size=ks, stride=stride)\n",
    "\n",
    "# main netspec wrapper\n",
    "def caffenet_multilabel(data_layer_params, datalayer):\n",
    "    # setup the python data layer \n",
    "    n = caffe.NetSpec()\n",
    "    n.data, n.image_label, n.pixel_label = L.Python(module = 'pascal_ss_datalayers', layer = datalayer, \n",
    "                               ntop = 3, param_str=str(data_layer_params))\n",
    "\n",
    "    # the net itself\n",
    "    n.conv1_1, n.relu1_1 = conv_relu(n.data, 3, 64, pad=100)\n",
    "    n.conv1_2, n.relu1_2 = conv_relu(n.relu1_1, 3, 64, pad=1)\n",
    "    n.pool1 = max_pool(n.relu1_2, 2, stride=2)\n",
    "    \n",
    "    n.conv2_1, n.relu2_1 = conv_relu(n.pool1, 3, 128, pad=1)\n",
    "    n.conv2_2, n.relu2_2 = conv_relu(n.relu2_1, 3, 128, pad=1)\n",
    "    n.pool2 = max_pool(n.relu2_2, 2, stride=2)\n",
    "\n",
    "    n.conv3_1, n.relu3_1 = conv_relu(n.pool2, 3, 256, pad=1)\n",
    "    n.conv3_2, n.relu3_2 = conv_relu(n.relu3_1, 3, 256, pad=1)\n",
    "    n.conv3_3, n.relu3_3 = conv_relu(n.relu3_2, 3, 256, pad=1)\n",
    "    n.pool3 = max_pool(n.relu3_3, 2, stride=2)    \n",
    "    \n",
    "    n.conv4_1, n.relu4_1 = conv_relu(n.pool3, 3, 512, pad=1)\n",
    "    n.conv4_2, n.relu4_2 = conv_relu(n.relu4_1, 3, 512, pad=1)\n",
    "    n.conv4_3, n.relu4_3 = conv_relu(n.relu4_2, 3, 512, pad=1)\n",
    "    n.pool4 = max_pool(n.relu4_3, 2, stride=2)     \n",
    "    \n",
    "    n.conv5_1, n.relu5_1 = conv_relu(n.pool4, 3, 512, pad=1)\n",
    "    n.conv5_2, n.relu5_2 = conv_relu(n.relu5_1, 3, 512, pad=1)\n",
    "    n.conv5_3, n.relu5_3 = conv_relu(n.relu5_2, 3, 512, pad=1)\n",
    "    n.pool5 = max_pool(n.relu5_3, 2, stride=2)\n",
    "    \n",
    "    n.fc6, n.relu6 = conv_relu(n.pool5, 7, 4096)\n",
    "    n.drop6 = L.Dropout(n.relu6, in_place=True)\n",
    "    n.fc7, n.relu7 = conv_relu(n.drop6, 1, 4096)\n",
    "    n.drop7 = L.Dropout(n.relu7, in_place=True)\n",
    "    n.score_fr, n.relu8 = conv_relu(n.drop7, 1, 21)    \n",
    "    \n",
    "    n.upsample = L.Deconvolution(n.score_fr, num_output=21, kernel_size=64, stride=32)\n",
    "    \n",
    "    n.score = L.Crop(n.upsample, n.data)#, n.data)\n",
    "    n.loss = L.Python(n.score, n.pixel_label, n.image_label, module = 'alaska_loss', layer = 'AlaskaLossLayer',  ntop = 1) # specify loss weight\n",
    "    \n",
    "    return str(n.to_proto())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = './secondNet'\n",
    "if not os.path.isdir(workdir):\n",
    "    os.makedirs(workdir)\n",
    "\n",
    "solverprototxt = tools.CaffeSolver(trainnet_prototxt_path = osp.join(workdir, \"trainnet.prototxt\"), testnet_prototxt_path = osp.join(workdir, \"valnet.prototxt\"))\n",
    "solverprototxt.sp['display'] = \"1\"\n",
    "solverprototxt.sp['base_lr'] = \"0.0001\"\n",
    "solverprototxt.write(osp.join(workdir, 'solver.prototxt'))\n",
    "\n",
    "# # write train net.\n",
    "# with open(osp.join(workdir, 'trainnet.prototxt'), 'w') as f:\n",
    "#     # provide parameters to the data layer as a python dictionary. Easy as pie!\n",
    "#     data_layer_params = dict(batch_size = 1, im_shape = [224, 224], split = 'train', pascal_root = pascal_root)\n",
    "#     f.write(caffenet_multilabel(data_layer_params, 'PascalMultilabelDataLayerSync'))\n",
    "\n",
    "# # write validation net.\n",
    "# with open(osp.join(workdir, 'valnet.prototxt'), 'w') as f:\n",
    "#     data_layer_params = dict(batch_size = 1, im_shape = [224, 224], split = 'val', pascal_root = pascal_root)\n",
    "#     f.write(caffenet_multilabel(data_layer_params, 'PascalMultilabelDataLayerSync'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
